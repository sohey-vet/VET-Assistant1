#!/usr/bin/env python3
"""
ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØÊ©üËÉΩ„É¢„Ç∏„É•„Éº„É´
ÊäïÁ®øÂÜÖÂÆπ„ÅÆÈáçË§á„ÇÑÈ°û‰ººÊÄß„ÇíÂé≥Èáç„Å´„ÉÅ„Çß„ÉÉ„ÇØ„Åô„Çã
"""

import json
import hashlib
import sqlite3
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import difflib
from collections import Counter

class DuplicateChecker:
    def __init__(self, db_path: str = "post_history.db"):
        self.db_path = Path(db_path)
        self.init_database()
    
    def init_database(self):
        """ÊäïÁ®øÂ±•Ê≠¥„Éá„Éº„Çø„Éô„Éº„Çπ„ÇíÂàùÊúüÂåñ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS post_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content TEXT NOT NULL,
                content_hash TEXT NOT NULL,
                normalized_content TEXT NOT NULL,
                topic TEXT,
                post_type TEXT,
                day TEXT,
                char_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                keywords TEXT,
                main_points TEXT
            )
        ''')
        
        # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàê
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_hash ON post_history(content_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON post_history(topic)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_post_type ON post_history(post_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON post_history(created_at)')
        
        conn.commit()
        conn.close()
    
    def normalize_content(self, content: str) -> str:
        """ÊäïÁ®øÂÜÖÂÆπ„ÇíÊ≠£Ë¶èÂåñÔºàÊØîËºÉÁî®Ôºâ"""
        # ÊîπË°å„ÄÅÁ©∫ÁôΩ„ÄÅÁµµÊñáÂ≠ó„ÄÅ„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÇíÈô§Âéª
        normalized = re.sub(r'[\n\r\s]', '', content)
        normalized = re.sub(r'[^\w\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', '', normalized)
        normalized = re.sub(r'#\w+', '', normalized)
        return normalized.lower()
    
    def extract_keywords(self, content: str) -> List[str]:
        """ÊäïÁ®øÂÜÖÂÆπ„Åã„Çâ„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÊäΩÂá∫"""
        # Â∞ÇÈñÄÁî®Ë™û„ÄÅÁóÖÂêç„ÄÅÁå´Á®ÆÂêç„Å™„Å©„ÇíÊäΩÂá∫
        keywords = []
        
        # ÁóÖÂêç„ÉªÁóáÁä∂„Éë„Çø„Éº„É≥
        disease_patterns = [
            r'(ËÖéËáìÁóÖ|ÂøÉËáìÁóÖ|Á≥ñÂ∞øÁóÖ|Áî≤Áä∂ËÖ∫|ËÇùËáì|ËÜÄËÉ±|Â∞øË∑Ø|ÁµêÁü≥|ÊÑüÊüìÁóá|„Ç¢„É¨„É´„ÇÆ„Éº)',
            r'(ÁôΩÂÜÖÈöú|Á∑ëÂÜÖÈöú|ÁµêËÜúÁÇé|ÁöÆËÜöÁÇé|Â§ñËÄ≥ÁÇé|Ê≠ØÂë®ÁóÖ|Âè£ÂÜÖÁÇé)',
            r'(ÂòîÂêê|‰∏ãÁó¢|‰æøÁßò|Áô∫ÁÜ±|È£üÊ¨≤‰∏çÊåØ|‰ΩìÈáçÊ∏õÂ∞ë|ÂëºÂê∏Âõ∞Èõ£)'
        ]
        
        # Áå´Á®Æ„Éë„Çø„Éº„É≥
        breed_patterns = [
            r'(„Ç¢„É°„É™„Ç´„É≥„Ç∑„Éß„Éº„Éà„Éò„Ç¢|„Éö„É´„Ç∑„É£|„É≠„Ç∑„Ç¢„É≥„Éñ„É´„Éº|„Çπ„Ç≥„ÉÜ„Ç£„ÉÉ„Ç∑„É•„Éï„Ç©„Éº„É´„Éâ)',
            r'(„É°„Ç§„É≥„ÇØ„Éº„É≥|„É©„Ç∞„Éâ„Éº„É´|„Éô„É≥„Ç¨„É´|„Ç¢„Éì„Ç∑„Éã„Ç¢„É≥|„Éû„É≥„ÉÅ„Ç´„É≥|„Éñ„É™„ÉÜ„Ç£„ÉÉ„Ç∑„É•)'
        ]
        
        # ÂåªÁôÇÁî®Ë™û„Éë„Çø„Éº„É≥
        medical_patterns = [
            r'(Ë®∫Êñ≠|Ê≤ªÁôÇ|ÊâãË°ì|Ëñ¨|„ÉØ„ÇØ„ÉÅ„É≥|Ê§úÊüª|Ë°ÄÊ∂≤Ê§úÊüª|„É¨„É≥„Éà„Ç≤„É≥|„Ç®„Ç≥„Éº)',
            r'(ÁóáÁä∂|‰∫àÈò≤|„Ç±„Ç¢|ÁÆ°ÁêÜ|Ë¶≥ÂØü|ÂØæÂá¶Ê≥ï|ÂøúÊÄ•Âá¶ÁΩÆ)'
        ]
        
        all_patterns = disease_patterns + breed_patterns + medical_patterns
        
        for pattern in all_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.extend(matches)
        
        return list(set(keywords))
    
    def extract_main_points(self, content: str) -> List[str]:
        """ÊäïÁ®øÂÜÖÂÆπ„Åã„Çâ‰∏ªË¶Å„Éù„Ç§„É≥„Éà„ÇíÊäΩÂá∫"""
        points = []
        
        # ÁÆáÊù°Êõ∏„Åç„Éù„Ç§„É≥„Éà„ÇíÊäΩÂá∫
        bullet_patterns = [
            r'‚úÖ\s*([^\n]+)',
            r'üí°\s*([^\n]+)', 
            r'üêæ\s*([^\n]+)',
            r'‚ö†Ô∏è\s*([^\n]+)',
            r'‚ùó\s*([^\n]+)'
        ]
        
        for pattern in bullet_patterns:
            matches = re.findall(pattern, content)
            points.extend(matches)
        
        # ÈáçË¶Å„Å™Êñá„ÇíÊäΩÂá∫
        important_sentences = re.findall(r'[„ÄÇÔºÅÔºü][^\n]*?[ÈáçË¶Å|Â§ßÂàá|Ê≥®ÊÑè|ÂøÖË¶Å|Êé®Â•®][^\n]*?[„ÄÇÔºÅÔºü]', content)
        points.extend(important_sentences)
        
        return [point.strip() for point in points if point.strip()]
    
    def calculate_content_hash(self, content: str) -> str:
        """ÊäïÁ®øÂÜÖÂÆπ„ÅÆ„Éè„ÉÉ„Ç∑„É•ÂÄ§„ÇíË®àÁÆó"""
        normalized = self.normalize_content(content)
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def calculate_similarity(self, content1: str, content2: str) -> float:
        """2„Å§„ÅÆÊäïÁ®øÂÜÖÂÆπ„ÅÆÈ°û‰ººÂ∫¶„ÇíË®àÁÆóÔºà0.0-1.0Ôºâ"""
        norm1 = self.normalize_content(content1)
        norm2 = self.normalize_content(content2)
        
        # ÂÆåÂÖ®‰∏ÄËá¥„ÅÆÂ†¥Âêà
        if norm1 == norm2:
            return 1.0
        
        # ÊñáÂ≠óÂàó„ÅÆÈ°û‰ººÂ∫¶
        text_similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()
        
        # „Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÈ°û‰ººÂ∫¶
        keywords1 = set(self.extract_keywords(content1))
        keywords2 = set(self.extract_keywords(content2))
        
        if keywords1 or keywords2:
            keyword_similarity = len(keywords1 & keywords2) / len(keywords1 | keywords2)
        else:
            keyword_similarity = 0.0
        
        # ‰∏ªË¶Å„Éù„Ç§„É≥„Éà„ÅÆÈ°û‰ººÂ∫¶
        points1 = set(self.extract_main_points(content1))
        points2 = set(self.extract_main_points(content2))
        
        if points1 or points2:
            points_similarity = len(points1 & points2) / len(points1 | points2)
        else:
            points_similarity = 0.0
        
        # Èáç„Åø‰ªò„ÅçÂπ≥Âùá
        final_similarity = (
            text_similarity * 0.4 +
            keyword_similarity * 0.4 +
            points_similarity * 0.2
        )
        
        return final_similarity
    
    def save_post(self, content: str, topic: str = "", post_type: str = "", day: str = "") -> bool:
        """ÊäïÁ®ø„ÇíÂ±•Ê≠¥„Å´‰øùÂ≠ò"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            content_hash = self.calculate_content_hash(content)
            normalized_content = self.normalize_content(content)
            keywords = json.dumps(self.extract_keywords(content), ensure_ascii=False)
            main_points = json.dumps(self.extract_main_points(content), ensure_ascii=False)
            char_count = len(content.replace('\n', ''))
            
            cursor.execute('''
                INSERT INTO post_history 
                (content, content_hash, normalized_content, topic, post_type, day, 
                 char_count, keywords, main_points)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (content, content_hash, normalized_content, topic, post_type, day,
                  char_count, keywords, main_points))
            
            conn.commit()
            conn.close()
            return True
            
        except Exception as e:
            print(f"‚ùå ÊäïÁ®ø‰øùÂ≠ò„Ç®„É©„Éº: {e}")
            return False
    
    def check_duplicate(self, content: str, topic: str = "", post_type: str = "", 
                       similarity_threshold: float = 0.7) -> Tuple[bool, List[Dict]]:
        """ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØ„ÇíÂÆüË°å"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ÂÆåÂÖ®‰∏ÄËá¥„ÉÅ„Çß„ÉÉ„ÇØ
            content_hash = self.calculate_content_hash(content)
            cursor.execute('SELECT * FROM post_history WHERE content_hash = ?', (content_hash,))
            exact_match = cursor.fetchone()
            
            if exact_match:
                return True, [{
                    'type': 'exact_match',
                    'similarity': 1.0,
                    'content': exact_match[1],
                    'topic': exact_match[3],
                    'created_at': exact_match[8]
                }]
            
            # È°û‰ººÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ
            duplicates = []
            
            # Âêå„Åò„Éà„Éî„ÉÉ„ÇØ„ÅÆÊäïÁ®ø„ÇíÂÑ™ÂÖàÁöÑ„Å´„ÉÅ„Çß„ÉÉ„ÇØ
            if topic:
                cursor.execute('''
                    SELECT * FROM post_history 
                    WHERE topic = ? 
                    ORDER BY created_at DESC 
                    LIMIT 20
                ''', (topic,))
                similar_topic_posts = cursor.fetchall()
                
                for post in similar_topic_posts:
                    similarity = self.calculate_similarity(content, post[1])
                    if similarity >= similarity_threshold:
                        duplicates.append({
                            'type': 'similar_topic',
                            'similarity': similarity,
                            'content': post[1],
                            'topic': post[3],
                            'created_at': post[8]
                        })
            
            # Âêå„ÅòÊäïÁ®ø„Çø„Ç§„Éó„ÅÆÊäïÁ®ø„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            if post_type:
                cursor.execute('''
                    SELECT * FROM post_history 
                    WHERE post_type = ? 
                    ORDER BY created_at DESC 
                    LIMIT 30
                ''', (post_type,))
                similar_type_posts = cursor.fetchall()
                
                for post in similar_type_posts:
                    similarity = self.calculate_similarity(content, post[1])
                    if similarity >= similarity_threshold:
                        # Êó¢„Å´ËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                        if not any(d['content'] == post[1] for d in duplicates):
                            duplicates.append({
                                'type': 'similar_type',
                                'similarity': similarity,
                                'content': post[1],
                                'topic': post[3],
                                'created_at': post[8]
                            })
            
            # ÊúÄËøë„ÅÆÊäïÁ®øÂÖ®Ëà¨„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            cursor.execute('''
                SELECT * FROM post_history 
                ORDER BY created_at DESC 
                LIMIT 50
            ''')
            recent_posts = cursor.fetchall()
            
            for post in recent_posts:
                similarity = self.calculate_similarity(content, post[1])
                if similarity >= similarity_threshold:
                    # Êó¢„Å´ËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                    if not any(d['content'] == post[1] for d in duplicates):
                        duplicates.append({
                            'type': 'recent_similar',
                            'similarity': similarity,
                            'content': post[1],
                            'topic': post[3],
                            'created_at': post[8]
                        })
            
            conn.close()
            
            # È°û‰ººÂ∫¶„Åß„ÇΩ„Éº„Éà
            duplicates.sort(key=lambda x: x['similarity'], reverse=True)
            
            return len(duplicates) > 0, duplicates
            
        except Exception as e:
            print(f"‚ùå ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {e}")
            return False, []
    
    def get_post_history(self, limit: int = 50) -> List[Dict]:
        """ÊäïÁ®øÂ±•Ê≠¥„ÇíÂèñÂæó"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT * FROM post_history 
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (limit,))
            
            posts = cursor.fetchall()
            conn.close()
            
            return [{
                'id': post[0],
                'content': post[1],
                'topic': post[3],
                'post_type': post[4],
                'day': post[5],
                'char_count': post[6],
                'created_at': post[8],
                'keywords': json.loads(post[9]) if post[9] else [],
                'main_points': json.loads(post[10]) if post[10] else []
            } for post in posts]
            
        except Exception as e:
            print(f"‚ùå Â±•Ê≠¥ÂèñÂæó„Ç®„É©„Éº: {e}")
            return []
    
    def clean_old_posts(self, days_to_keep: int = 90):
        """Âè§„ÅÑÊäïÁ®ø„ÇíÂâäÈô§"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                DELETE FROM post_history 
                WHERE created_at < datetime('now', '-{} days')
            '''.format(days_to_keep))
            
            deleted_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            print(f"‚úÖ {deleted_count}‰ª∂„ÅÆÂè§„ÅÑÊäïÁ®ø„ÇíÂâäÈô§„Åó„Åæ„Åó„Åü")
            return True
            
        except Exception as e:
            print(f"‚ùå Âè§„ÅÑÊäïÁ®øÂâäÈô§„Ç®„É©„Éº: {e}")
            return False