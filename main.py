#!/usr/bin/env python3
"""
VET-ASSISTANT-CLI: AIæŠ•ç¨¿ç”Ÿæˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
@souhei1219ã®XæŠ•ç¨¿ã‚’åŠè‡ªå‹•ç”Ÿæˆã™ã‚‹CLIãƒ„ãƒ¼ãƒ«
"""

import os
import sys
import json
import argparse
import re
from datetime import datetime
from pathlib import Path
import google.generativeai as genai
from duplicate_checker import DuplicateChecker

class VetAssistantCLI:
    def __init__(self):
        self.config_file = Path("config.json")
        self.persona_file = Path("persona_data.json")
        self.api_key = "AIzaSyAA0eEtEXToBEtZSrdllKJYZdkHQDrfgik"
        self.persona_data = {}
        self.duplicate_checker = DuplicateChecker()
        self.load_config()
        self.setup_gemini()
    
    def load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        else:
            self.config = {
                "x_archive_path": "",
                "learned": False,
                "last_post_type": "",
                "current_week": 1,
                "current_cycle": 1
            }
            self.save_config()
    
    def save_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
    
    def setup_gemini(self):
        """Gemini APIã®è¨­å®š"""
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-pro')
    
    def count_characters(self, text):
        """
        æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ”¹è¡Œé™¤å¤–ï¼‰
        çµµæ–‡å­—ãƒ»è¨˜å·ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å«ã‚€å…¨è§’æ–‡å­—ã‚’1æ–‡å­—ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
        """
        # æ”¹è¡Œã‚’é™¤å¤–
        text_without_newlines = text.replace('\n', '')
        return len(text_without_newlines)
    
    def load_x_archive(self, archive_path):
        """Xã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            tweets_file = Path(archive_path) / "data" / "tweets.js"
            if not tweets_file.exists():
                print(f"âŒ tweets.jsãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tweets_file}")
                return None
            
            with open(tweets_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                json_start = content.find('[')
                json_data = content[json_start:]
                tweets = json.loads(json_data)
                print(f"âœ… {len(tweets)}ä»¶ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                return tweets
        except Exception as e:
            print(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    
    def analyze_persona(self, tweets):
        """æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒšãƒ«ã‚½ãƒŠã‚’åˆ†æ"""
        print("ğŸ” ãƒšãƒ«ã‚½ãƒŠåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        tweet_texts = []
        for tweet in tweets:
            if 'tweet' in tweet and 'full_text' in tweet['tweet']:
                tweet_texts.append(tweet['tweet']['full_text'])
        
        # æœ€æ–°ã®1000ä»¶ã‚’åˆ†æå¯¾è±¡ã¨ã™ã‚‹
        recent_tweets = tweet_texts[:1000]
        sample_text = '\n'.join(recent_tweets)
        
        # Geminiã§ãƒšãƒ«ã‚½ãƒŠåˆ†æ
        prompt = f"""
ä»¥ä¸‹ã¯æ•‘æ€¥ç£åŒ»å¸«@souhei1219ã®XæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€ä»¥ä¸‹ã®è¦ç´ ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

1. åŸºæœ¬çš„ãªæ–‡ä½“ãƒ»å£èª¿ã®ç‰¹å¾´
2. ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹çµµæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
3. å°‚é–€ç”¨èªã®è¨€ã„æ›ãˆãƒ‘ã‚¿ãƒ¼ãƒ³
4. æ–‡ç« æ§‹æˆã®ç‰¹å¾´
5. æŠ•ç¨¿ã®å‚¾å‘ãƒ»ãƒ†ãƒ¼ãƒ

æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿:
{sample_text[:5000]}

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.model.generate_content(prompt)
            # JSONã‚’æŠ½å‡º
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                self.persona_data = json.loads(json_match.group())
                self.save_persona()
                print("âœ… ãƒšãƒ«ã‚½ãƒŠåˆ†æå®Œäº†")
                return True
            else:
                print("âŒ ãƒšãƒ«ã‚½ãƒŠåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
        except Exception as e:
            print(f"âŒ ãƒšãƒ«ã‚½ãƒŠåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def save_persona(self):
        """ãƒšãƒ«ã‚½ãƒŠãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        with open(self.persona_file, 'w', encoding='utf-8') as f:
            json.dump(self.persona_data, f, ensure_ascii=False, indent=2)
    
    def load_persona(self):
        """ãƒšãƒ«ã‚½ãƒŠãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if self.persona_file.exists():
            with open(self.persona_file, 'r', encoding='utf-8') as f:
                self.persona_data = json.load(f)
                return True
        return False
    
    def learn_command(self, archive_path):
        """learnã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œ"""
        print("ğŸ“ ãƒšãƒ«ã‚½ãƒŠå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # Xã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’èª­ã¿è¾¼ã¿
        tweets = self.load_x_archive(archive_path)
        if not tweets:
            return False
        
        # ãƒšãƒ«ã‚½ãƒŠã‚’åˆ†æ
        if self.analyze_persona(tweets):
            self.config["x_archive_path"] = archive_path
            self.config["learned"] = True
            self.save_config()
            print("ğŸ‰ ãƒšãƒ«ã‚½ãƒŠå­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ•‘æ€¥ç£åŒ»å¸« @souhei1219 ã¨ã—ã¦ã®æ€è€ƒã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            return True
        
        return False
    
    def get_post_cycle_info(self):
        """ç¾åœ¨ã®æŠ•ç¨¿ã‚µã‚¤ã‚¯ãƒ«æƒ…å ±ã‚’å–å¾—"""
        week = self.config.get("current_week", 1)
        cycle = self.config.get("current_cycle", 1)
        
        # 4é€±ã‚µã‚¤ã‚¯ãƒ«ã®åˆ¤å®š
        if week == 1:
            return "çŒ«ç¨®ç‰¹é›†é€±"
        elif week == 2:
            return "å°‚é–€ãƒ†ãƒ¼ãƒé€±"
        else:
            return "å‚åŠ å‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¼·åŒ–é€±"
    
    def generate_post(self, post_type, day, topic, max_attempts=3):
        """æŠ•ç¨¿ã‚’ç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        if not self.config.get("learned", False):
            print("âŒ å…ˆã«learnã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return None
        
        if not self.load_persona():
            print("âŒ ãƒšãƒ«ã‚½ãƒŠãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å«ã‚€æŠ•ç¨¿ç”Ÿæˆã‚’è©¦è¡Œ
        for attempt in range(max_attempts):
            print(f"ğŸ”„ æŠ•ç¨¿ç”Ÿæˆè©¦è¡Œ {attempt + 1}/{max_attempts}")
            
            post_content, char_count = self._generate_single_post(post_type, day, topic)
            if not post_content:
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            print("ğŸ” é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
            is_duplicate, duplicate_info = self.duplicate_checker.check_duplicate(
                post_content, topic, post_type, similarity_threshold=0.7
            )
            
            if not is_duplicate:
                print("âœ… é‡è¤‡ãªã— - æŠ•ç¨¿ã‚’æ‰¿èª")
                # æŠ•ç¨¿ã‚’å±¥æ­´ã«ä¿å­˜
                self.duplicate_checker.save_post(post_content, topic, post_type, day)
                return post_content, char_count
            else:
                print(f"âš ï¸ é‡è¤‡æ¤œå‡º - {len(duplicate_info)}ä»¶ã®é¡ä¼¼æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                self._show_duplicate_info(duplicate_info)
                
                if attempt < max_attempts - 1:
                    print("ğŸ”„ æŠ•ç¨¿ã‚’å†ç”Ÿæˆã—ã¾ã™...")
                    # ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤ºã§å†ç”Ÿæˆ
                    topic = f"{topic} (åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)"
        
        print("âŒ é‡è¤‡ã®ãªã„æŠ•ç¨¿ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None, 0
        
    def _generate_single_post(self, post_type, day, topic):
        """å˜ä¸€æŠ•ç¨¿ã‚’ç”Ÿæˆï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰"""
        # æ›œæ—¥ã‚’æ—¥æœ¬èªã«å¤‰æ›
        day_map = {
            "mon": "æœˆæ›œæ—¥", "tue": "ç«æ›œæ—¥", "wed": "æ°´æ›œæ—¥", 
            "thu": "æœ¨æ›œæ—¥", "fri": "é‡‘æ›œæ—¥", "sat": "åœŸæ›œæ—¥", "sun": "æ—¥æ›œæ—¥"
        }
        day_jp = day_map.get(day, day)
        
        # æŠ•ç¨¿ã‚µã‚¤ã‚¯ãƒ«æƒ…å ±ã‚’å–å¾—
        cycle_info = self.get_post_cycle_info()
        
        # æ—¢å­˜ã®æŠ•ç¨¿å±¥æ­´ã‚’å–å¾—ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹æŒ‡ç¤ºã‚’è¿½åŠ 
        recent_posts = self.duplicate_checker.get_post_history(10)
        recent_topics = [post['topic'] for post in recent_posts if post['topic']]
        recent_keywords = []
        for post in recent_posts:
            recent_keywords.extend(post.get('keywords', []))
        
        avoid_instruction = ""
        if recent_topics:
            avoid_instruction += f"\n\n## é‡è¤‡å›é¿æŒ‡ç¤º\næœ€è¿‘ã®æŠ•ç¨¿ãƒˆãƒ”ãƒƒã‚¯: {', '.join(recent_topics[:5])}\n"
        if recent_keywords:
            common_keywords = [k for k, v in Counter(recent_keywords).most_common(10)]
            avoid_instruction += f"ã‚ˆãä½¿ã‚ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(common_keywords)}\n"
            avoid_instruction += "ä¸Šè¨˜ã®ãƒˆãƒ”ãƒƒã‚¯ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã€æ–°ã—ã„è¦–ç‚¹ã‚„è¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        
        # æŠ•ç¨¿ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""
ã‚ãªãŸã¯19å¹´ç›®ã®æ•‘æ€¥ç£åŒ»å¸«ï¼ˆçŠ¬çŒ«å°‚é–€ï¼‰@souhei1219ã§ã™ã€‚ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦æŠ•ç¨¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
1. æŠ•ç¨¿ã¯140æ–‡å­—ä»¥å†…ï¼ˆæ”¹è¡Œé™¤ãã€çµµæ–‡å­—ãƒ»è¨˜å·ãƒ»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å«ã‚€ï¼‰
2. å¿…ãšã€Œ#çŒ«ã®ã‚ã‚Œã“ã‚Œã€ã‚’ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¨ã—ã¦å«ã‚ã‚‹
3. ã€Œç£åŒ»å¸«ãŒæ•™ãˆã‚‹ï¼ã€ãƒˆãƒ”ãƒƒã‚¯åã€‘ã€çµµæ–‡å­—ã€‘ã€ã®å½¢å¼ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½œæˆ
4. å°‚é–€ç”¨èªã¯é«˜æ ¡ç”Ÿã§ã‚‚ç†è§£ã§ãã‚‹è¨€è‘‰ã«è¨€ã„æ›ãˆã‚‹
5. ç®‡æ¡æ›¸ãï¼ˆâœ…ã€ğŸ’¡ã€ğŸ¾ãªã©ï¼‰ã‚’åŠ¹æœçš„ã«ä½¿ç”¨
6. æ”¹è¡Œã‚’ä½¿ã£ã¦èª­ã¿ã‚„ã™ãã™ã‚‹
7. éå»ã®æŠ•ç¨¿ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã€ç‹¬è‡ªæ€§ã®ã‚ã‚‹å†…å®¹ã«ã™ã‚‹

## æŠ•ç¨¿æ¡ä»¶
- æŠ•ç¨¿ã‚¿ã‚¤ãƒ—: {post_type}
- æ›œæ—¥: {day_jp}
- ãƒˆãƒ”ãƒƒã‚¯: {topic}
- ç¾åœ¨ã®ã‚µã‚¤ã‚¯ãƒ«: {cycle_info}

## ãƒšãƒ«ã‚½ãƒŠãƒ‡ãƒ¼ã‚¿
{json.dumps(self.persona_data, ensure_ascii=False, indent=2)}

## æŠ•ç¨¿ã‚µã‚¤ã‚¯ãƒ«ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
çŒ«ç¨®ç‰¹é›†é€±ã®{day_jp}ã®å ´åˆã¯ä»¥ä¸‹ã®å†…å®¹ã§:
- æœˆæ›œ: çŒ«ç¨®ã®æ¦‚è¦ç´¹ä»‹
- ç«æ›œ: æ­´å²ãƒ»èµ·æº
- æ°´æ›œ: æ€§æ ¼
- æœ¨æ›œ: ä½“å‹ã‚„è¢«æ¯›ãªã©ã®èº«ä½“çš„ç‰¹å¾´
- é‡‘æ›œ: ç‰¹æœ‰ã®å¥åº·ä¸Šã®æ³¨æ„ç‚¹
- åœŸæ›œ: æ—¥å¸¸çš„ãªã‚±ã‚¢ã®ãƒã‚¤ãƒ³ãƒˆ
- æ—¥æ›œ: å‚åŠ å‹ã®æŠ•ç¨¿

å°‚é–€ãƒ†ãƒ¼ãƒé€±ã®{day_jp}ã®å ´åˆã¯ä»¥ä¸‹ã®å†…å®¹ã§:
- æœˆæ›œ: ãƒ†ãƒ¼ãƒã®æ¦‚è¦
- ç«æ›œ: åŸå› ã‚„åˆæœŸç—‡çŠ¶
- æ°´æ›œ: é€²è¡Œæ™‚ã®ç—‡çŠ¶ã‚„åˆä½µç—‡
- æœ¨æ›œ: è¨ºæ–­æ–¹æ³•
- é‡‘æ›œ: æ²»ç™‚ãƒ»ç®¡ç†æ–¹æ³•
- åœŸæ›œ: ãŠå®¶ã§ã§ãã‚‹ã‚±ã‚¢
- æ—¥æ›œ: ç·æ‹¬ã€äºˆé˜²ã®é‡è¦æ€§

å‚åŠ å‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¼·åŒ–é€±ã¯èª­è€…ã¨ã®äº¤æµã‚’é‡è¦–ã—ãŸå†…å®¹ã§ã€‚{avoid_instruction}

æŠ•ç¨¿æ–‡ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.model.generate_content(prompt)
            post_content = response.text.strip()
            
            # æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
            char_count = self.count_characters(post_content)
            
            if char_count > 140:
                print(f"âš ï¸ æ–‡å­—æ•°ã‚ªãƒ¼ãƒãƒ¼ï¼ˆ{char_count}æ–‡å­—ï¼‰ã§ã™ã€‚å†ç”Ÿæˆã—ã¾ã™...")
                # çŸ­ç¸®ç‰ˆã‚’ç”Ÿæˆ
                short_prompt = prompt + "\n\nâ€»140æ–‡å­—ã‚’è¶…éã—ã¾ã—ãŸã€‚ã‚ˆã‚Šç°¡æ½”ã«ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                response = self.model.generate_content(short_prompt)
                post_content = response.text.strip()
                char_count = self.count_characters(post_content)
            
            return post_content, char_count
        
        except Exception as e:
            print(f"âŒ æŠ•ç¨¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None, 0
    
    def x_post_command(self, post_type, day, topic):
        """x-postã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œ"""
        print(f"ğŸ“ æŠ•ç¨¿ã‚’ç”Ÿæˆä¸­... (ã‚¿ã‚¤ãƒ—: {post_type}, æ›œæ—¥: {day}, ãƒˆãƒ”ãƒƒã‚¯: {topic})")
        
        result = self.generate_post(post_type, day, topic)
        if result:
            post_content, char_count = result
            print("\n" + "="*50)
            print("ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸæŠ•ç¨¿:")
            print("="*50)
            print(post_content)
            print("="*50)
            print(f"æ–‡å­—æ•°: ({char_count}æ–‡å­—)")
            print("="*50)
            
            # è¨­å®šæ›´æ–°
            self.config["last_post_type"] = post_type
            self.save_config()
            
            return post_content
        else:
            print("âŒ æŠ•ç¨¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
    
    def _show_duplicate_info(self, duplicate_info):
        """é‡è¤‡æƒ…å ±ã‚’è¡¨ç¤º"""
        print("\nğŸ“‹ é‡è¤‡æ¤œå‡ºè©³ç´°:")
        print("="*60)
        
        for i, dup in enumerate(duplicate_info[:3], 1):  # ä¸Šä½3ä»¶ã‚’è¡¨ç¤º
            print(f"\n{i}. é¡ä¼¼åº¦: {dup['similarity']:.2f} ({dup['type']})")
            print(f"   ãƒˆãƒ”ãƒƒã‚¯: {dup['topic']}")
            print(f"   ä½œæˆæ—¥: {dup['created_at']}")
            print(f"   å†…å®¹: {dup['content'][:50]}{'...' if len(dup['content']) > 50 else ''}")
        
        print("="*60)
    
    def show_post_history(self, limit=10):
        """æŠ•ç¨¿å±¥æ­´ã‚’è¡¨ç¤º"""
        posts = self.duplicate_checker.get_post_history(limit)
        
        if not posts:
            print("ğŸ“ æŠ•ç¨¿å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print(f"\nğŸ“‹ æœ€è¿‘ã®æŠ•ç¨¿å±¥æ­´ (æœ€æ–°{len(posts)}ä»¶):")
        print("="*80)
        
        for post in posts:
            print(f"\nğŸ“… {post['created_at']}")
            print(f"ğŸ“ ãƒˆãƒ”ãƒƒã‚¯: {post['topic']}")
            print(f"ğŸ·ï¸  ã‚¿ã‚¤ãƒ—: {post['post_type']} ({post['day']})")
            print(f"ğŸ“Š æ–‡å­—æ•°: {post['char_count']}")
            print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(post['keywords'][:5])}")
            print(f"ğŸ’¬ å†…å®¹: {post['content'][:60]}{'...' if len(post['content']) > 60 else ''}")
            print("-" * 80)
    
    def clean_old_posts(self, days=90):
        """å¤ã„æŠ•ç¨¿ã‚’å‰Šé™¤"""
        return self.duplicate_checker.clean_old_posts(days)

def main():
    parser = argparse.ArgumentParser(description="VET-ASSISTANT-CLI: AIæŠ•ç¨¿ç”Ÿæˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³")
    subparsers = parser.add_subparsers(dest='command', help='åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰')
    
    # learnã‚³ãƒãƒ³ãƒ‰
    learn_parser = subparsers.add_parser('learn', help='ãƒšãƒ«ã‚½ãƒŠå­¦ç¿’')
    learn_parser.add_argument('archive_path', help='Xã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹')
    
    # x-postã‚³ãƒãƒ³ãƒ‰
    xpost_parser = subparsers.add_parser('x-post', help='æŠ•ç¨¿ç”Ÿæˆ')
    xpost_parser.add_argument('--type', choices=['cat-breed', 'specialty', 'interactive'], 
                             required=True, help='æŠ•ç¨¿ã‚¿ã‚¤ãƒ—')
    xpost_parser.add_argument('--day', choices=['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun'], 
                             required=True, help='æ›œæ—¥')
    xpost_parser.add_argument('--topic', required=True, help='ãƒˆãƒ”ãƒƒã‚¯')
    
    # historyã‚³ãƒãƒ³ãƒ‰
    history_parser = subparsers.add_parser('history', help='æŠ•ç¨¿å±¥æ­´è¡¨ç¤º')
    history_parser.add_argument('--limit', type=int, default=10, help='è¡¨ç¤ºä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰')
    
    # cleanã‚³ãƒãƒ³ãƒ‰
    clean_parser = subparsers.add_parser('clean', help='å¤ã„æŠ•ç¨¿å±¥æ­´ã‚’å‰Šé™¤')
    clean_parser.add_argument('--days', type=int, default=90, help='ä¿æŒæ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 90æ—¥ï¼‰')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    app = VetAssistantCLI()
    
    if args.command == 'learn':
        app.learn_command(args.archive_path)
    elif args.command == 'x-post':
        app.x_post_command(args.type, args.day, args.topic)
    elif args.command == 'history':
        app.show_post_history(args.limit)
    elif args.command == 'clean':
        app.clean_old_posts(args.days)

if __name__ == "__main__":
    main()